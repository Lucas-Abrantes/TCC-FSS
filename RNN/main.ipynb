{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('credit.pkl', 'rb') as f:\n",
    "    X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_credit_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_teste.shape, y_credit_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.43724252\n",
      "Iteration 2, loss = 1.41084662\n",
      "Iteration 3, loss = 1.38457412\n",
      "Iteration 4, loss = 1.35910559\n",
      "Iteration 5, loss = 1.33468248\n",
      "Iteration 6, loss = 1.31076680\n",
      "Iteration 7, loss = 1.28754668\n",
      "Iteration 8, loss = 1.26498123\n",
      "Iteration 9, loss = 1.24347257\n",
      "Iteration 10, loss = 1.22308570\n",
      "Iteration 11, loss = 1.20345697\n",
      "Iteration 12, loss = 1.18459604\n",
      "Iteration 13, loss = 1.16659320\n",
      "Iteration 14, loss = 1.14937844\n",
      "Iteration 15, loss = 1.13322108\n",
      "Iteration 16, loss = 1.11761690\n",
      "Iteration 17, loss = 1.10277563\n",
      "Iteration 18, loss = 1.08857440\n",
      "Iteration 19, loss = 1.07489693\n",
      "Iteration 20, loss = 1.06181005\n",
      "Iteration 21, loss = 1.04898377\n",
      "Iteration 22, loss = 1.03649721\n",
      "Iteration 23, loss = 1.02421226\n",
      "Iteration 24, loss = 1.01227552\n",
      "Iteration 25, loss = 1.00075565\n",
      "Iteration 26, loss = 0.98930060\n",
      "Iteration 27, loss = 0.97824344\n",
      "Iteration 28, loss = 0.96748126\n",
      "Iteration 29, loss = 0.95696548\n",
      "Iteration 30, loss = 0.94653772\n",
      "Iteration 31, loss = 0.93648843\n",
      "Iteration 32, loss = 0.92654367\n",
      "Iteration 33, loss = 0.91681966\n",
      "Iteration 34, loss = 0.90756735\n",
      "Iteration 35, loss = 0.89869179\n",
      "Iteration 36, loss = 0.89003354\n",
      "Iteration 37, loss = 0.88211093\n",
      "Iteration 38, loss = 0.87461905\n",
      "Iteration 39, loss = 0.86749201\n",
      "Iteration 40, loss = 0.86077248\n",
      "Iteration 41, loss = 0.85450764\n",
      "Iteration 42, loss = 0.84878988\n",
      "Iteration 43, loss = 0.84351341\n",
      "Iteration 44, loss = 0.83874786\n",
      "Iteration 45, loss = 0.83447983\n",
      "Iteration 46, loss = 0.83052694\n",
      "Iteration 47, loss = 0.82684147\n",
      "Iteration 48, loss = 0.82330282\n",
      "Iteration 49, loss = 0.81992263\n",
      "Iteration 50, loss = 0.81667158\n",
      "Iteration 51, loss = 0.81347705\n",
      "Iteration 52, loss = 0.81034868\n",
      "Iteration 53, loss = 0.80729152\n",
      "Iteration 54, loss = 0.80426199\n",
      "Iteration 55, loss = 0.80128891\n",
      "Iteration 56, loss = 0.79831385\n",
      "Iteration 57, loss = 0.79539485\n",
      "Iteration 58, loss = 0.79245163\n",
      "Iteration 59, loss = 0.78960053\n",
      "Iteration 60, loss = 0.78673108\n",
      "Iteration 61, loss = 0.78386675\n",
      "Iteration 62, loss = 0.78105994\n",
      "Iteration 63, loss = 0.77826997\n",
      "Iteration 64, loss = 0.77547150\n",
      "Iteration 65, loss = 0.77272980\n",
      "Iteration 66, loss = 0.76998151\n",
      "Iteration 67, loss = 0.76725604\n",
      "Iteration 68, loss = 0.76451269\n",
      "Iteration 69, loss = 0.76184032\n",
      "Iteration 70, loss = 0.75914920\n",
      "Iteration 71, loss = 0.75652581\n",
      "Iteration 72, loss = 0.75385644\n",
      "Iteration 73, loss = 0.75125962\n",
      "Iteration 74, loss = 0.74863058\n",
      "Iteration 75, loss = 0.74604478\n",
      "Iteration 76, loss = 0.74348208\n",
      "Iteration 77, loss = 0.74090992\n",
      "Iteration 78, loss = 0.73833625\n",
      "Iteration 79, loss = 0.73580613\n",
      "Iteration 80, loss = 0.73328196\n",
      "Iteration 81, loss = 0.73078652\n",
      "Iteration 82, loss = 0.72825790\n",
      "Iteration 83, loss = 0.72579632\n",
      "Iteration 84, loss = 0.72331770\n",
      "Iteration 85, loss = 0.72087801\n",
      "Iteration 86, loss = 0.71843136\n",
      "Iteration 87, loss = 0.71603034\n",
      "Iteration 88, loss = 0.71360751\n",
      "Iteration 89, loss = 0.71122109\n",
      "Iteration 90, loss = 0.70882367\n",
      "Iteration 91, loss = 0.70645503\n",
      "Iteration 92, loss = 0.70411453\n",
      "Iteration 93, loss = 0.70177329\n",
      "Iteration 94, loss = 0.69945415\n",
      "Iteration 95, loss = 0.69713428\n",
      "Iteration 96, loss = 0.69483158\n",
      "Iteration 97, loss = 0.69255899\n",
      "Iteration 98, loss = 0.69033570\n",
      "Iteration 99, loss = 0.68808490\n",
      "Iteration 100, loss = 0.68583746\n",
      "Iteration 101, loss = 0.68363912\n",
      "Iteration 102, loss = 0.68140627\n",
      "Iteration 103, loss = 0.67921920\n",
      "Iteration 104, loss = 0.67701949\n",
      "Iteration 105, loss = 0.67488623\n",
      "Iteration 106, loss = 0.67271225\n",
      "Iteration 107, loss = 0.67059946\n",
      "Iteration 108, loss = 0.66847465\n",
      "Iteration 109, loss = 0.66634979\n",
      "Iteration 110, loss = 0.66427088\n",
      "Iteration 111, loss = 0.66215135\n",
      "Iteration 112, loss = 0.66008393\n",
      "Iteration 113, loss = 0.65800955\n",
      "Iteration 114, loss = 0.65591114\n",
      "Iteration 115, loss = 0.65382943\n",
      "Iteration 116, loss = 0.65176241\n",
      "Iteration 117, loss = 0.64969649\n",
      "Iteration 118, loss = 0.64761040\n",
      "Iteration 119, loss = 0.64555261\n",
      "Iteration 120, loss = 0.64345271\n",
      "Iteration 121, loss = 0.64135144\n",
      "Iteration 122, loss = 0.63923458\n",
      "Iteration 123, loss = 0.63704658\n",
      "Iteration 124, loss = 0.63486932\n",
      "Iteration 125, loss = 0.63255655\n",
      "Iteration 126, loss = 0.63029511\n",
      "Iteration 127, loss = 0.62786128\n",
      "Iteration 128, loss = 0.62540603\n",
      "Iteration 129, loss = 0.62277766\n",
      "Iteration 130, loss = 0.62007377\n",
      "Iteration 131, loss = 0.61712067\n",
      "Iteration 132, loss = 0.61402677\n",
      "Iteration 133, loss = 0.61059712\n",
      "Iteration 134, loss = 0.60690236\n",
      "Iteration 135, loss = 0.60282602\n",
      "Iteration 136, loss = 0.59839421\n",
      "Iteration 137, loss = 0.59349115\n",
      "Iteration 138, loss = 0.58793979\n",
      "Iteration 139, loss = 0.58199822\n",
      "Iteration 140, loss = 0.57531511\n",
      "Iteration 141, loss = 0.56802416\n",
      "Iteration 142, loss = 0.55988690\n",
      "Iteration 143, loss = 0.55121434\n",
      "Iteration 144, loss = 0.54169106\n",
      "Iteration 145, loss = 0.53170991\n",
      "Iteration 146, loss = 0.52104936\n",
      "Iteration 147, loss = 0.51010162\n",
      "Iteration 148, loss = 0.49891524\n",
      "Iteration 149, loss = 0.48754292\n",
      "Iteration 150, loss = 0.47612083\n",
      "Iteration 151, loss = 0.46498267\n",
      "Iteration 152, loss = 0.45404407\n",
      "Iteration 153, loss = 0.44332119\n",
      "Iteration 154, loss = 0.43321771\n",
      "Iteration 155, loss = 0.42345259\n",
      "Iteration 156, loss = 0.41433118\n",
      "Iteration 157, loss = 0.40638489\n",
      "Iteration 158, loss = 0.39848448\n",
      "Iteration 159, loss = 0.39140468\n",
      "Iteration 160, loss = 0.38469115\n",
      "Iteration 161, loss = 0.37830133\n",
      "Iteration 162, loss = 0.37208290\n",
      "Iteration 163, loss = 0.36620653\n",
      "Iteration 164, loss = 0.36042890\n",
      "Iteration 165, loss = 0.35484554\n",
      "Iteration 166, loss = 0.34947453\n",
      "Iteration 167, loss = 0.34422511\n",
      "Iteration 168, loss = 0.33909370\n",
      "Iteration 169, loss = 0.33418712\n",
      "Iteration 170, loss = 0.32934988\n",
      "Iteration 171, loss = 0.32464119\n",
      "Iteration 172, loss = 0.32009342\n",
      "Iteration 173, loss = 0.31565995\n",
      "Iteration 174, loss = 0.31140039\n",
      "Iteration 175, loss = 0.30717205\n",
      "Iteration 176, loss = 0.30310923\n",
      "Iteration 177, loss = 0.29911047\n",
      "Iteration 178, loss = 0.29522868\n",
      "Iteration 179, loss = 0.29147986\n",
      "Iteration 180, loss = 0.28784871\n",
      "Iteration 181, loss = 0.28428917\n",
      "Iteration 182, loss = 0.28089501\n",
      "Iteration 183, loss = 0.27762800\n",
      "Iteration 184, loss = 0.27452212\n",
      "Iteration 185, loss = 0.27145038\n",
      "Iteration 186, loss = 0.26853509\n",
      "Iteration 187, loss = 0.26572238\n",
      "Iteration 188, loss = 0.26298657\n",
      "Iteration 189, loss = 0.26045626\n",
      "Iteration 190, loss = 0.25803131\n",
      "Iteration 191, loss = 0.25568267\n",
      "Iteration 192, loss = 0.25349290\n",
      "Iteration 193, loss = 0.25133963\n",
      "Iteration 194, loss = 0.24930234\n",
      "Iteration 195, loss = 0.24734492\n",
      "Iteration 196, loss = 0.24544924\n",
      "Iteration 197, loss = 0.24360646\n",
      "Iteration 198, loss = 0.24183950\n",
      "Iteration 199, loss = 0.24011457\n",
      "Iteration 200, loss = 0.23850378\n",
      "Iteration 201, loss = 0.23695285\n",
      "Iteration 202, loss = 0.23540400\n",
      "Iteration 203, loss = 0.23396714\n",
      "Iteration 204, loss = 0.23255025\n",
      "Iteration 205, loss = 0.23120639\n",
      "Iteration 206, loss = 0.22990117\n",
      "Iteration 207, loss = 0.22864312\n",
      "Iteration 208, loss = 0.22745200\n",
      "Iteration 209, loss = 0.22627848\n",
      "Iteration 210, loss = 0.22514775\n",
      "Iteration 211, loss = 0.22407953\n",
      "Iteration 212, loss = 0.22303451\n",
      "Iteration 213, loss = 0.22201795\n",
      "Iteration 214, loss = 0.22103448\n",
      "Iteration 215, loss = 0.22005699\n",
      "Iteration 216, loss = 0.21911844\n",
      "Iteration 217, loss = 0.21822246\n",
      "Iteration 218, loss = 0.21733868\n",
      "Iteration 219, loss = 0.21649056\n",
      "Iteration 220, loss = 0.21566431\n",
      "Iteration 221, loss = 0.21485783\n",
      "Iteration 222, loss = 0.21406950\n",
      "Iteration 223, loss = 0.21332274\n",
      "Iteration 224, loss = 0.21256604\n",
      "Iteration 225, loss = 0.21182236\n",
      "Iteration 226, loss = 0.21109035\n",
      "Iteration 227, loss = 0.21037860\n",
      "Iteration 228, loss = 0.20969581\n",
      "Iteration 229, loss = 0.20902766\n",
      "Iteration 230, loss = 0.20838135\n",
      "Iteration 231, loss = 0.20774438\n",
      "Iteration 232, loss = 0.20715212\n",
      "Iteration 233, loss = 0.20653888\n",
      "Iteration 234, loss = 0.20596954\n",
      "Iteration 235, loss = 0.20541081\n",
      "Iteration 236, loss = 0.20483441\n",
      "Iteration 237, loss = 0.20430994\n",
      "Iteration 238, loss = 0.20377959\n",
      "Iteration 239, loss = 0.20325133\n",
      "Iteration 240, loss = 0.20273131\n",
      "Iteration 241, loss = 0.20224279\n",
      "Iteration 242, loss = 0.20175675\n",
      "Iteration 243, loss = 0.20128005\n",
      "Iteration 244, loss = 0.20080937\n",
      "Iteration 245, loss = 0.20033649\n",
      "Iteration 246, loss = 0.19993769\n",
      "Iteration 247, loss = 0.19944168\n",
      "Iteration 248, loss = 0.19900719\n",
      "Iteration 249, loss = 0.19856539\n",
      "Iteration 250, loss = 0.19815289\n",
      "Iteration 251, loss = 0.19773442\n",
      "Iteration 252, loss = 0.19731140\n",
      "Iteration 253, loss = 0.19691580\n",
      "Iteration 254, loss = 0.19652031\n",
      "Iteration 255, loss = 0.19611913\n",
      "Iteration 256, loss = 0.19573369\n",
      "Iteration 257, loss = 0.19535986\n",
      "Iteration 258, loss = 0.19498133\n",
      "Iteration 259, loss = 0.19463965\n",
      "Iteration 260, loss = 0.19425616\n",
      "Iteration 261, loss = 0.19390158\n",
      "Iteration 262, loss = 0.19355363\n",
      "Iteration 263, loss = 0.19323914\n",
      "Iteration 264, loss = 0.19288587\n",
      "Iteration 265, loss = 0.19253327\n",
      "Iteration 266, loss = 0.19220206\n",
      "Iteration 267, loss = 0.19187762\n",
      "Iteration 268, loss = 0.19156065\n",
      "Iteration 269, loss = 0.19123698\n",
      "Iteration 270, loss = 0.19091921\n",
      "Iteration 271, loss = 0.19061017\n",
      "Iteration 272, loss = 0.19031005\n",
      "Iteration 273, loss = 0.19001270\n",
      "Iteration 274, loss = 0.18970990\n",
      "Iteration 275, loss = 0.18941648\n",
      "Iteration 276, loss = 0.18911366\n",
      "Iteration 277, loss = 0.18881030\n",
      "Iteration 278, loss = 0.18852959\n",
      "Iteration 279, loss = 0.18824251\n",
      "Iteration 280, loss = 0.18795194\n",
      "Iteration 281, loss = 0.18764667\n",
      "Iteration 282, loss = 0.18736094\n",
      "Iteration 283, loss = 0.18708271\n",
      "Iteration 284, loss = 0.18680254\n",
      "Iteration 285, loss = 0.18652169\n",
      "Iteration 286, loss = 0.18625230\n",
      "Iteration 287, loss = 0.18598992\n",
      "Iteration 288, loss = 0.18572294\n",
      "Iteration 289, loss = 0.18544984\n",
      "Iteration 290, loss = 0.18517699\n",
      "Iteration 291, loss = 0.18490801\n",
      "Iteration 292, loss = 0.18466780\n",
      "Iteration 293, loss = 0.18440231\n",
      "Iteration 294, loss = 0.18414114\n",
      "Iteration 295, loss = 0.18388142\n",
      "Iteration 296, loss = 0.18363114\n",
      "Iteration 297, loss = 0.18338222\n",
      "Iteration 298, loss = 0.18312778\n",
      "Iteration 299, loss = 0.18288711\n",
      "Iteration 300, loss = 0.18264271\n",
      "Iteration 301, loss = 0.18239688\n",
      "Iteration 302, loss = 0.18216068\n",
      "Iteration 303, loss = 0.18192157\n",
      "Iteration 304, loss = 0.18167253\n",
      "Iteration 305, loss = 0.18143981\n",
      "Iteration 306, loss = 0.18119452\n",
      "Iteration 307, loss = 0.18095831\n",
      "Iteration 308, loss = 0.18073759\n",
      "Iteration 309, loss = 0.18049995\n",
      "Iteration 310, loss = 0.18027230\n",
      "Iteration 311, loss = 0.18003010\n",
      "Iteration 312, loss = 0.17982245\n",
      "Iteration 313, loss = 0.17958331\n",
      "Iteration 314, loss = 0.17934932\n",
      "Iteration 315, loss = 0.17912711\n",
      "Iteration 316, loss = 0.17891332\n",
      "Iteration 317, loss = 0.17866335\n",
      "Iteration 318, loss = 0.17848739\n",
      "Iteration 319, loss = 0.17823754\n",
      "Iteration 320, loss = 0.17801814\n",
      "Iteration 321, loss = 0.17779681\n",
      "Iteration 322, loss = 0.17758015\n",
      "Iteration 323, loss = 0.17739556\n",
      "Iteration 324, loss = 0.17718718\n",
      "Iteration 325, loss = 0.17692803\n",
      "Iteration 326, loss = 0.17670593\n",
      "Iteration 327, loss = 0.17649816\n",
      "Iteration 328, loss = 0.17627534\n",
      "Iteration 329, loss = 0.17610127\n",
      "Iteration 330, loss = 0.17586300\n",
      "Iteration 331, loss = 0.17564272\n",
      "Iteration 332, loss = 0.17544089\n",
      "Iteration 333, loss = 0.17524367\n",
      "Iteration 334, loss = 0.17501119\n",
      "Iteration 335, loss = 0.17478141\n",
      "Iteration 336, loss = 0.17457754\n",
      "Iteration 337, loss = 0.17437292\n",
      "Iteration 338, loss = 0.17416260\n",
      "Iteration 339, loss = 0.17395264\n",
      "Iteration 340, loss = 0.17374600\n",
      "Iteration 341, loss = 0.17354004\n",
      "Iteration 342, loss = 0.17333970\n",
      "Iteration 343, loss = 0.17314329\n",
      "Iteration 344, loss = 0.17293385\n",
      "Iteration 345, loss = 0.17276790\n",
      "Iteration 346, loss = 0.17254109\n",
      "Iteration 347, loss = 0.17234257\n",
      "Iteration 348, loss = 0.17214701\n",
      "Iteration 349, loss = 0.17192532\n",
      "Iteration 350, loss = 0.17172532\n",
      "Iteration 351, loss = 0.17155041\n",
      "Iteration 352, loss = 0.17133337\n",
      "Iteration 353, loss = 0.17113912\n",
      "Iteration 354, loss = 0.17093891\n",
      "Iteration 355, loss = 0.17075250\n",
      "Iteration 356, loss = 0.17054101\n",
      "Iteration 357, loss = 0.17035675\n",
      "Iteration 358, loss = 0.17017204\n",
      "Iteration 359, loss = 0.17004149\n",
      "Iteration 360, loss = 0.16978381\n",
      "Iteration 361, loss = 0.16958168\n",
      "Iteration 362, loss = 0.16938668\n",
      "Iteration 363, loss = 0.16920710\n",
      "Iteration 364, loss = 0.16900696\n",
      "Iteration 365, loss = 0.16883620\n",
      "Iteration 366, loss = 0.16863209\n",
      "Iteration 367, loss = 0.16843776\n",
      "Iteration 368, loss = 0.16824483\n",
      "Iteration 369, loss = 0.16803881\n",
      "Iteration 370, loss = 0.16786174\n",
      "Iteration 371, loss = 0.16766696\n",
      "Iteration 372, loss = 0.16749057\n",
      "Iteration 373, loss = 0.16729157\n",
      "Iteration 374, loss = 0.16709434\n",
      "Iteration 375, loss = 0.16690314\n",
      "Iteration 376, loss = 0.16672898\n",
      "Iteration 377, loss = 0.16651465\n",
      "Iteration 378, loss = 0.16635401\n",
      "Iteration 379, loss = 0.16616482\n",
      "Iteration 380, loss = 0.16595883\n",
      "Iteration 381, loss = 0.16576895\n",
      "Iteration 382, loss = 0.16558529\n",
      "Iteration 383, loss = 0.16537507\n",
      "Iteration 384, loss = 0.16516707\n",
      "Iteration 385, loss = 0.16502401\n",
      "Iteration 386, loss = 0.16480779\n",
      "Iteration 387, loss = 0.16457977\n",
      "Iteration 388, loss = 0.16440704\n",
      "Iteration 389, loss = 0.16417306\n",
      "Iteration 390, loss = 0.16398788\n",
      "Iteration 391, loss = 0.16381154\n",
      "Iteration 392, loss = 0.16361695\n",
      "Iteration 393, loss = 0.16342463\n",
      "Iteration 394, loss = 0.16324110\n",
      "Iteration 395, loss = 0.16307348\n",
      "Iteration 396, loss = 0.16286241\n",
      "Iteration 397, loss = 0.16266430\n",
      "Iteration 398, loss = 0.16249707\n",
      "Iteration 399, loss = 0.16231896\n",
      "Iteration 400, loss = 0.16210595\n",
      "Iteration 401, loss = 0.16192117\n",
      "Iteration 402, loss = 0.16174004\n",
      "Iteration 403, loss = 0.16154662\n",
      "Iteration 404, loss = 0.16136079\n",
      "Iteration 405, loss = 0.16119306\n",
      "Iteration 406, loss = 0.16098286\n",
      "Iteration 407, loss = 0.16080233\n",
      "Iteration 408, loss = 0.16062558\n",
      "Iteration 409, loss = 0.16043630\n",
      "Iteration 410, loss = 0.16026005\n",
      "Iteration 411, loss = 0.16007180\n",
      "Iteration 412, loss = 0.15988243\n",
      "Iteration 413, loss = 0.15968752\n",
      "Iteration 414, loss = 0.15951611\n",
      "Iteration 415, loss = 0.15934895\n",
      "Iteration 416, loss = 0.15913226\n",
      "Iteration 417, loss = 0.15895515\n",
      "Iteration 418, loss = 0.15880241\n",
      "Iteration 419, loss = 0.15862895\n",
      "Iteration 420, loss = 0.15842817\n",
      "Iteration 421, loss = 0.15824254\n",
      "Iteration 422, loss = 0.15806777\n",
      "Iteration 423, loss = 0.15790170\n",
      "Iteration 424, loss = 0.15773479\n",
      "Iteration 425, loss = 0.15751173\n",
      "Iteration 426, loss = 0.15738508\n",
      "Iteration 427, loss = 0.15715047\n",
      "Iteration 428, loss = 0.15698079\n",
      "Iteration 429, loss = 0.15679699\n",
      "Iteration 430, loss = 0.15661384\n",
      "Iteration 431, loss = 0.15642618\n",
      "Iteration 432, loss = 0.15623822\n",
      "Iteration 433, loss = 0.15606861\n",
      "Iteration 434, loss = 0.15586336\n",
      "Iteration 435, loss = 0.15568401\n",
      "Iteration 436, loss = 0.15551228\n",
      "Iteration 437, loss = 0.15533905\n",
      "Iteration 438, loss = 0.15513054\n",
      "Iteration 439, loss = 0.15498248\n",
      "Iteration 440, loss = 0.15478744\n",
      "Iteration 441, loss = 0.15461226\n",
      "Iteration 442, loss = 0.15443763\n",
      "Iteration 443, loss = 0.15422896\n",
      "Iteration 444, loss = 0.15409122\n",
      "Iteration 445, loss = 0.15388261\n",
      "Iteration 446, loss = 0.15370162\n",
      "Iteration 447, loss = 0.15349645\n",
      "Iteration 448, loss = 0.15331690\n",
      "Iteration 449, loss = 0.15312873\n",
      "Iteration 450, loss = 0.15292690\n",
      "Iteration 451, loss = 0.15273477\n",
      "Iteration 452, loss = 0.15253833\n",
      "Iteration 453, loss = 0.15234719\n",
      "Iteration 454, loss = 0.15218755\n",
      "Iteration 455, loss = 0.15197267\n",
      "Iteration 456, loss = 0.15176828\n",
      "Iteration 457, loss = 0.15157613\n",
      "Iteration 458, loss = 0.15143346\n",
      "Iteration 459, loss = 0.15121375\n",
      "Iteration 460, loss = 0.15101668\n",
      "Iteration 461, loss = 0.15082894\n",
      "Iteration 462, loss = 0.15061708\n",
      "Iteration 463, loss = 0.15044399\n",
      "Iteration 464, loss = 0.15024031\n",
      "Iteration 465, loss = 0.15004156\n",
      "Iteration 466, loss = 0.14984109\n",
      "Iteration 467, loss = 0.14968881\n",
      "Iteration 468, loss = 0.14948756\n",
      "Iteration 469, loss = 0.14930654\n",
      "Iteration 470, loss = 0.14908712\n",
      "Iteration 471, loss = 0.14892655\n",
      "Iteration 472, loss = 0.14869897\n",
      "Iteration 473, loss = 0.14850997\n",
      "Iteration 474, loss = 0.14828285\n",
      "Iteration 475, loss = 0.14808238\n",
      "Iteration 476, loss = 0.14789747\n",
      "Iteration 477, loss = 0.14763865\n",
      "Iteration 478, loss = 0.14752339\n",
      "Iteration 479, loss = 0.14721731\n",
      "Iteration 480, loss = 0.14702061\n",
      "Iteration 481, loss = 0.14682419\n",
      "Iteration 482, loss = 0.14658122\n",
      "Iteration 483, loss = 0.14637218\n",
      "Iteration 484, loss = 0.14618525\n",
      "Iteration 485, loss = 0.14593031\n",
      "Iteration 486, loss = 0.14570422\n",
      "Iteration 487, loss = 0.14548958\n",
      "Iteration 488, loss = 0.14525683\n",
      "Iteration 489, loss = 0.14503711\n",
      "Iteration 490, loss = 0.14480514\n",
      "Iteration 491, loss = 0.14460430\n",
      "Iteration 492, loss = 0.14434604\n",
      "Iteration 493, loss = 0.14416293\n",
      "Iteration 494, loss = 0.14391992\n",
      "Iteration 495, loss = 0.14367541\n",
      "Iteration 496, loss = 0.14348124\n",
      "Iteration 497, loss = 0.14324474\n",
      "Iteration 498, loss = 0.14301693\n",
      "Iteration 499, loss = 0.14279257\n",
      "Iteration 500, loss = 0.14256453\n",
      "Iteration 501, loss = 0.14232893\n",
      "Iteration 502, loss = 0.14212605\n",
      "Iteration 503, loss = 0.14188209\n",
      "Iteration 504, loss = 0.14167474\n",
      "Iteration 505, loss = 0.14142499\n",
      "Iteration 506, loss = 0.14120059\n",
      "Iteration 507, loss = 0.14098411\n",
      "Iteration 508, loss = 0.14075295\n",
      "Iteration 509, loss = 0.14053561\n",
      "Iteration 510, loss = 0.14032395\n",
      "Iteration 511, loss = 0.14010038\n",
      "Iteration 512, loss = 0.13990747\n",
      "Iteration 513, loss = 0.13966970\n",
      "Iteration 514, loss = 0.13949806\n",
      "Iteration 515, loss = 0.13923645\n",
      "Iteration 516, loss = 0.13903931\n",
      "Iteration 517, loss = 0.13880286\n",
      "Iteration 518, loss = 0.13858933\n",
      "Iteration 519, loss = 0.13835376\n",
      "Iteration 520, loss = 0.13814239\n",
      "Iteration 521, loss = 0.13794317\n",
      "Iteration 522, loss = 0.13770072\n",
      "Iteration 523, loss = 0.13749104\n",
      "Iteration 524, loss = 0.13726525\n",
      "Iteration 525, loss = 0.13705264\n",
      "Iteration 526, loss = 0.13681373\n",
      "Iteration 527, loss = 0.13660753\n",
      "Iteration 528, loss = 0.13637326\n",
      "Iteration 529, loss = 0.13616428\n",
      "Iteration 530, loss = 0.13593775\n",
      "Iteration 531, loss = 0.13571507\n",
      "Iteration 532, loss = 0.13556018\n",
      "Iteration 533, loss = 0.13527348\n",
      "Iteration 534, loss = 0.13503924\n",
      "Iteration 535, loss = 0.13483259\n",
      "Iteration 536, loss = 0.13459971\n",
      "Iteration 537, loss = 0.13437415\n",
      "Iteration 538, loss = 0.13418837\n",
      "Iteration 539, loss = 0.13392800\n",
      "Iteration 540, loss = 0.13376140\n",
      "Iteration 541, loss = 0.13353740\n",
      "Iteration 542, loss = 0.13327124\n",
      "Iteration 543, loss = 0.13305772\n",
      "Iteration 544, loss = 0.13282339\n",
      "Iteration 545, loss = 0.13264290\n",
      "Iteration 546, loss = 0.13239543\n",
      "Iteration 547, loss = 0.13218899\n",
      "Iteration 548, loss = 0.13195144\n",
      "Iteration 549, loss = 0.13173224\n",
      "Iteration 550, loss = 0.13149711\n",
      "Iteration 551, loss = 0.13130622\n",
      "Iteration 552, loss = 0.13109154\n",
      "Iteration 553, loss = 0.13085068\n",
      "Iteration 554, loss = 0.13062704\n",
      "Iteration 555, loss = 0.13039912\n",
      "Iteration 556, loss = 0.13014677\n",
      "Iteration 557, loss = 0.12990908\n",
      "Iteration 558, loss = 0.12969600\n",
      "Iteration 559, loss = 0.12946699\n",
      "Iteration 560, loss = 0.12930496\n",
      "Iteration 561, loss = 0.12899216\n",
      "Iteration 562, loss = 0.12877610\n",
      "Iteration 563, loss = 0.12853129\n",
      "Iteration 564, loss = 0.12832083\n",
      "Iteration 565, loss = 0.12815461\n",
      "Iteration 566, loss = 0.12790259\n",
      "Iteration 567, loss = 0.12763224\n",
      "Iteration 568, loss = 0.12737352\n",
      "Iteration 569, loss = 0.12716683\n",
      "Iteration 570, loss = 0.12692372\n",
      "Iteration 571, loss = 0.12669197\n",
      "Iteration 572, loss = 0.12646501\n",
      "Iteration 573, loss = 0.12626148\n",
      "Iteration 574, loss = 0.12602051\n",
      "Iteration 575, loss = 0.12578549\n",
      "Iteration 576, loss = 0.12554743\n",
      "Iteration 577, loss = 0.12536218\n",
      "Iteration 578, loss = 0.12511647\n",
      "Iteration 579, loss = 0.12485876\n",
      "Iteration 580, loss = 0.12462797\n",
      "Iteration 581, loss = 0.12444880\n",
      "Iteration 582, loss = 0.12417341\n",
      "Iteration 583, loss = 0.12396919\n",
      "Iteration 584, loss = 0.12371845\n",
      "Iteration 585, loss = 0.12351067\n",
      "Iteration 586, loss = 0.12322683\n",
      "Iteration 587, loss = 0.12298912\n",
      "Iteration 588, loss = 0.12277045\n",
      "Iteration 589, loss = 0.12253377\n",
      "Iteration 590, loss = 0.12228211\n",
      "Iteration 591, loss = 0.12205198\n",
      "Iteration 592, loss = 0.12181475\n",
      "Iteration 593, loss = 0.12157195\n",
      "Iteration 594, loss = 0.12132770\n",
      "Iteration 595, loss = 0.12114156\n",
      "Iteration 596, loss = 0.12090892\n",
      "Iteration 597, loss = 0.12062684\n",
      "Iteration 598, loss = 0.12045112\n",
      "Iteration 599, loss = 0.12015647\n",
      "Iteration 600, loss = 0.11995539\n",
      "Iteration 601, loss = 0.11967268\n",
      "Iteration 602, loss = 0.11949316\n",
      "Iteration 603, loss = 0.11921568\n",
      "Iteration 604, loss = 0.11897141\n",
      "Iteration 605, loss = 0.11873878\n",
      "Iteration 606, loss = 0.11848749\n",
      "Iteration 607, loss = 0.11824603\n",
      "Iteration 608, loss = 0.11801924\n",
      "Iteration 609, loss = 0.11777668\n",
      "Iteration 610, loss = 0.11756251\n",
      "Iteration 611, loss = 0.11737629\n",
      "Iteration 612, loss = 0.11705963\n",
      "Iteration 613, loss = 0.11693583\n",
      "Iteration 614, loss = 0.11661845\n",
      "Iteration 615, loss = 0.11635562\n",
      "Iteration 616, loss = 0.11617562\n",
      "Iteration 617, loss = 0.11587895\n",
      "Iteration 618, loss = 0.11564224\n",
      "Iteration 619, loss = 0.11548922\n",
      "Iteration 620, loss = 0.11524356\n",
      "Iteration 621, loss = 0.11494082\n",
      "Iteration 622, loss = 0.11467650\n",
      "Iteration 623, loss = 0.11443520\n",
      "Iteration 624, loss = 0.11420405\n",
      "Iteration 625, loss = 0.11399734\n",
      "Iteration 626, loss = 0.11372572\n",
      "Iteration 627, loss = 0.11347576\n",
      "Iteration 628, loss = 0.11324255\n",
      "Iteration 629, loss = 0.11295447\n",
      "Iteration 630, loss = 0.11271753\n",
      "Iteration 631, loss = 0.11243407\n",
      "Iteration 632, loss = 0.11218315\n",
      "Iteration 633, loss = 0.11196223\n",
      "Iteration 634, loss = 0.11170510\n",
      "Iteration 635, loss = 0.11144935\n",
      "Iteration 636, loss = 0.11116973\n",
      "Iteration 637, loss = 0.11093739\n",
      "Iteration 638, loss = 0.11069642\n",
      "Iteration 639, loss = 0.11049286\n",
      "Iteration 640, loss = 0.11013315\n",
      "Iteration 641, loss = 0.10989154\n",
      "Iteration 642, loss = 0.10962302\n",
      "Iteration 643, loss = 0.10936689\n",
      "Iteration 644, loss = 0.10912309\n",
      "Iteration 645, loss = 0.10888259\n",
      "Iteration 646, loss = 0.10854878\n",
      "Iteration 647, loss = 0.10827517\n",
      "Iteration 648, loss = 0.10800937\n",
      "Iteration 649, loss = 0.10773988\n",
      "Iteration 650, loss = 0.10746688\n",
      "Iteration 651, loss = 0.10720555\n",
      "Iteration 652, loss = 0.10691910\n",
      "Iteration 653, loss = 0.10663634\n",
      "Iteration 654, loss = 0.10633076\n",
      "Iteration 655, loss = 0.10604384\n",
      "Iteration 656, loss = 0.10577482\n",
      "Iteration 657, loss = 0.10549589\n",
      "Iteration 658, loss = 0.10524752\n",
      "Iteration 659, loss = 0.10497769\n",
      "Iteration 660, loss = 0.10462352\n",
      "Iteration 661, loss = 0.10435770\n",
      "Iteration 662, loss = 0.10406981\n",
      "Iteration 663, loss = 0.10380003\n",
      "Iteration 664, loss = 0.10351441\n",
      "Iteration 665, loss = 0.10322109\n",
      "Iteration 666, loss = 0.10300553\n",
      "Iteration 667, loss = 0.10269859\n",
      "Iteration 668, loss = 0.10242150\n",
      "Iteration 669, loss = 0.10216221\n",
      "Iteration 670, loss = 0.10188328\n",
      "Iteration 671, loss = 0.10159904\n",
      "Iteration 672, loss = 0.10133365\n",
      "Iteration 673, loss = 0.10104142\n",
      "Iteration 674, loss = 0.10072608\n",
      "Iteration 675, loss = 0.10046968\n",
      "Iteration 676, loss = 0.10020040\n",
      "Iteration 677, loss = 0.09990243\n",
      "Iteration 678, loss = 0.09963670\n",
      "Iteration 679, loss = 0.09938586\n",
      "Iteration 680, loss = 0.09913108\n",
      "Iteration 681, loss = 0.09884735\n",
      "Iteration 682, loss = 0.09858930\n",
      "Iteration 683, loss = 0.09830663\n",
      "Iteration 684, loss = 0.09808514\n",
      "Iteration 685, loss = 0.09777049\n",
      "Iteration 686, loss = 0.09751159\n",
      "Iteration 687, loss = 0.09722277\n",
      "Iteration 688, loss = 0.09696223\n",
      "Iteration 689, loss = 0.09669124\n",
      "Iteration 690, loss = 0.09642986\n",
      "Iteration 691, loss = 0.09617241\n",
      "Iteration 692, loss = 0.09594487\n",
      "Iteration 693, loss = 0.09563008\n",
      "Iteration 694, loss = 0.09537321\n",
      "Iteration 695, loss = 0.09510720\n",
      "Iteration 696, loss = 0.09481143\n",
      "Iteration 697, loss = 0.09451880\n",
      "Iteration 698, loss = 0.09416330\n",
      "Iteration 699, loss = 0.09385808\n",
      "Iteration 700, loss = 0.09355114\n",
      "Iteration 701, loss = 0.09325828\n",
      "Iteration 702, loss = 0.09289802\n",
      "Iteration 703, loss = 0.09253728\n",
      "Iteration 704, loss = 0.09222543\n",
      "Iteration 705, loss = 0.09196024\n",
      "Iteration 706, loss = 0.09164136\n",
      "Iteration 707, loss = 0.09133270\n",
      "Iteration 708, loss = 0.09103478\n",
      "Iteration 709, loss = 0.09072771\n",
      "Iteration 710, loss = 0.09042875\n",
      "Iteration 711, loss = 0.09011551\n",
      "Iteration 712, loss = 0.08985877\n",
      "Iteration 713, loss = 0.08956159\n",
      "Iteration 714, loss = 0.08935352\n",
      "Iteration 715, loss = 0.08899970\n",
      "Iteration 716, loss = 0.08868720\n",
      "Iteration 717, loss = 0.08838048\n",
      "Iteration 718, loss = 0.08809376\n",
      "Iteration 719, loss = 0.08778858\n",
      "Iteration 720, loss = 0.08746508\n",
      "Iteration 721, loss = 0.08714955\n",
      "Iteration 722, loss = 0.08686382\n",
      "Iteration 723, loss = 0.08662861\n",
      "Iteration 724, loss = 0.08637627\n",
      "Iteration 725, loss = 0.08601377\n",
      "Iteration 726, loss = 0.08577101\n",
      "Iteration 727, loss = 0.08546715\n",
      "Iteration 728, loss = 0.08521443\n",
      "Iteration 729, loss = 0.08489605\n",
      "Iteration 730, loss = 0.08461684\n",
      "Iteration 731, loss = 0.08437326\n",
      "Iteration 732, loss = 0.08409522\n",
      "Iteration 733, loss = 0.08378705\n",
      "Iteration 734, loss = 0.08353279\n",
      "Iteration 735, loss = 0.08319214\n",
      "Iteration 736, loss = 0.08290367\n",
      "Iteration 737, loss = 0.08272232\n",
      "Iteration 738, loss = 0.08235297\n",
      "Iteration 739, loss = 0.08209661\n",
      "Iteration 740, loss = 0.08178040\n",
      "Iteration 741, loss = 0.08148709\n",
      "Iteration 742, loss = 0.08121838\n",
      "Iteration 743, loss = 0.08093018\n",
      "Iteration 744, loss = 0.08066340\n",
      "Iteration 745, loss = 0.08041545\n",
      "Iteration 746, loss = 0.08012311\n",
      "Iteration 747, loss = 0.07983645\n",
      "Iteration 748, loss = 0.07956061\n",
      "Iteration 749, loss = 0.07928096\n",
      "Iteration 750, loss = 0.07901310\n",
      "Iteration 751, loss = 0.07874312\n",
      "Iteration 752, loss = 0.07852380\n",
      "Iteration 753, loss = 0.07821246\n",
      "Iteration 754, loss = 0.07796615\n",
      "Iteration 755, loss = 0.07771495\n",
      "Iteration 756, loss = 0.07745719\n",
      "Iteration 757, loss = 0.07719464\n",
      "Iteration 758, loss = 0.07708422\n",
      "Iteration 759, loss = 0.07672181\n",
      "Iteration 760, loss = 0.07637932\n",
      "Iteration 761, loss = 0.07616196\n",
      "Iteration 762, loss = 0.07588450\n",
      "Iteration 763, loss = 0.07562294\n",
      "Iteration 764, loss = 0.07538193\n",
      "Iteration 765, loss = 0.07512022\n",
      "Iteration 766, loss = 0.07486575\n",
      "Iteration 767, loss = 0.07457234\n",
      "Iteration 768, loss = 0.07438999\n",
      "Iteration 769, loss = 0.07407985\n",
      "Iteration 770, loss = 0.07383691\n",
      "Iteration 771, loss = 0.07360418\n",
      "Iteration 772, loss = 0.07332978\n",
      "Iteration 773, loss = 0.07308489\n",
      "Iteration 774, loss = 0.07287547\n",
      "Iteration 775, loss = 0.07259542\n",
      "Iteration 776, loss = 0.07236987\n",
      "Iteration 777, loss = 0.07209756\n",
      "Iteration 778, loss = 0.07189774\n",
      "Iteration 779, loss = 0.07163384\n",
      "Iteration 780, loss = 0.07138990\n",
      "Iteration 781, loss = 0.07112217\n",
      "Iteration 782, loss = 0.07086885\n",
      "Iteration 783, loss = 0.07066422\n",
      "Iteration 784, loss = 0.07038070\n",
      "Iteration 785, loss = 0.07018121\n",
      "Iteration 786, loss = 0.06990787\n",
      "Iteration 787, loss = 0.06965661\n",
      "Iteration 788, loss = 0.06942132\n",
      "Iteration 789, loss = 0.06918294\n",
      "Iteration 790, loss = 0.06894250\n",
      "Iteration 791, loss = 0.06869600\n",
      "Iteration 792, loss = 0.06850095\n",
      "Iteration 793, loss = 0.06823604\n",
      "Iteration 794, loss = 0.06804043\n",
      "Iteration 795, loss = 0.06779307\n",
      "Iteration 796, loss = 0.06756132\n",
      "Iteration 797, loss = 0.06731478\n",
      "Iteration 798, loss = 0.06715463\n",
      "Iteration 799, loss = 0.06688904\n",
      "Iteration 800, loss = 0.06671270\n",
      "Iteration 801, loss = 0.06642703\n",
      "Iteration 802, loss = 0.06622380\n",
      "Iteration 803, loss = 0.06608051\n",
      "Iteration 804, loss = 0.06579206\n",
      "Iteration 805, loss = 0.06559283\n",
      "Iteration 806, loss = 0.06537493\n",
      "Iteration 807, loss = 0.06519041\n",
      "Iteration 808, loss = 0.06493134\n",
      "Iteration 809, loss = 0.06473581\n",
      "Iteration 810, loss = 0.06451102\n",
      "Iteration 811, loss = 0.06435614\n",
      "Iteration 812, loss = 0.06406833\n",
      "Iteration 813, loss = 0.06386510\n",
      "Iteration 814, loss = 0.06365997\n",
      "Iteration 815, loss = 0.06348692\n",
      "Iteration 816, loss = 0.06328391\n",
      "Iteration 817, loss = 0.06304683\n",
      "Iteration 818, loss = 0.06284273\n",
      "Iteration 819, loss = 0.06261248\n",
      "Iteration 820, loss = 0.06240905\n",
      "Iteration 821, loss = 0.06221205\n",
      "Iteration 822, loss = 0.06199666\n",
      "Iteration 823, loss = 0.06178798\n",
      "Iteration 824, loss = 0.06160458\n",
      "Iteration 825, loss = 0.06142269\n",
      "Iteration 826, loss = 0.06116815\n",
      "Iteration 827, loss = 0.06096707\n",
      "Iteration 828, loss = 0.06081806\n",
      "Iteration 829, loss = 0.06057818\n",
      "Iteration 830, loss = 0.06035432\n",
      "Iteration 831, loss = 0.06016395\n",
      "Iteration 832, loss = 0.05995500\n",
      "Iteration 833, loss = 0.05973065\n",
      "Iteration 834, loss = 0.05958630\n",
      "Iteration 835, loss = 0.05939443\n",
      "Iteration 836, loss = 0.05916180\n",
      "Iteration 837, loss = 0.05897911\n",
      "Iteration 838, loss = 0.05874759\n",
      "Iteration 839, loss = 0.05856148\n",
      "Iteration 840, loss = 0.05837778\n",
      "Iteration 841, loss = 0.05824392\n",
      "Iteration 842, loss = 0.05798250\n",
      "Iteration 843, loss = 0.05778699\n",
      "Iteration 844, loss = 0.05762916\n",
      "Iteration 845, loss = 0.05746293\n",
      "Iteration 846, loss = 0.05720148\n",
      "Iteration 847, loss = 0.05700755\n",
      "Iteration 848, loss = 0.05683569\n",
      "Iteration 849, loss = 0.05663030\n",
      "Iteration 850, loss = 0.05644323\n",
      "Iteration 851, loss = 0.05624499\n",
      "Iteration 852, loss = 0.05605152\n",
      "Iteration 853, loss = 0.05587797\n",
      "Iteration 854, loss = 0.05577395\n",
      "Iteration 855, loss = 0.05553179\n",
      "Iteration 856, loss = 0.05530559\n",
      "Iteration 857, loss = 0.05516498\n",
      "Iteration 858, loss = 0.05497750\n",
      "Iteration 859, loss = 0.05474447\n",
      "Iteration 860, loss = 0.05457675\n",
      "Iteration 861, loss = 0.05438745\n",
      "Iteration 862, loss = 0.05420814\n",
      "Iteration 863, loss = 0.05402140\n",
      "Iteration 864, loss = 0.05384516\n",
      "Iteration 865, loss = 0.05366412\n",
      "Iteration 866, loss = 0.05348713\n",
      "Iteration 867, loss = 0.05330560\n",
      "Iteration 868, loss = 0.05312758\n",
      "Iteration 869, loss = 0.05298059\n",
      "Iteration 870, loss = 0.05276513\n",
      "Iteration 871, loss = 0.05260560\n",
      "Iteration 872, loss = 0.05247176\n",
      "Iteration 873, loss = 0.05226991\n",
      "Iteration 874, loss = 0.05204975\n",
      "Iteration 875, loss = 0.05186737\n",
      "Iteration 876, loss = 0.05169148\n",
      "Iteration 877, loss = 0.05161070\n",
      "Iteration 878, loss = 0.05134988\n",
      "Iteration 879, loss = 0.05121610\n",
      "Iteration 880, loss = 0.05102561\n",
      "Iteration 881, loss = 0.05086537\n",
      "Iteration 882, loss = 0.05068354\n",
      "Iteration 883, loss = 0.05051795\n",
      "Iteration 884, loss = 0.05033371\n",
      "Iteration 885, loss = 0.05018645\n",
      "Iteration 886, loss = 0.04998424\n",
      "Iteration 887, loss = 0.04987281\n",
      "Iteration 888, loss = 0.04964240\n",
      "Iteration 889, loss = 0.04950563\n",
      "Iteration 890, loss = 0.04940061\n",
      "Iteration 891, loss = 0.04916607\n",
      "Iteration 892, loss = 0.04904379\n",
      "Iteration 893, loss = 0.04882894\n",
      "Iteration 894, loss = 0.04864690\n",
      "Iteration 895, loss = 0.04848052\n",
      "Iteration 896, loss = 0.04836092\n",
      "Iteration 897, loss = 0.04818383\n",
      "Iteration 898, loss = 0.04801443\n",
      "Iteration 899, loss = 0.04785147\n",
      "Iteration 900, loss = 0.04768533\n",
      "Iteration 901, loss = 0.04755053\n",
      "Iteration 902, loss = 0.04737932\n",
      "Iteration 903, loss = 0.04724191\n",
      "Iteration 904, loss = 0.04707571\n",
      "Iteration 905, loss = 0.04695297\n",
      "Iteration 906, loss = 0.04679170\n",
      "Iteration 907, loss = 0.04663897\n",
      "Iteration 908, loss = 0.04649817\n",
      "Iteration 909, loss = 0.04639248\n",
      "Iteration 910, loss = 0.04618169\n",
      "Iteration 911, loss = 0.04607017\n",
      "Iteration 912, loss = 0.04595287\n",
      "Iteration 913, loss = 0.04575515\n",
      "Iteration 914, loss = 0.04567815\n",
      "Iteration 915, loss = 0.04546728\n",
      "Iteration 916, loss = 0.04532349\n",
      "Iteration 917, loss = 0.04517159\n",
      "Iteration 918, loss = 0.04504038\n",
      "Iteration 919, loss = 0.04490679\n",
      "Iteration 920, loss = 0.04477023\n",
      "Iteration 921, loss = 0.04462513\n",
      "Iteration 922, loss = 0.04449472\n",
      "Iteration 923, loss = 0.04439176\n",
      "Iteration 924, loss = 0.04420238\n",
      "Iteration 925, loss = 0.04408346\n",
      "Iteration 926, loss = 0.04391803\n",
      "Iteration 927, loss = 0.04377793\n",
      "Iteration 928, loss = 0.04364970\n",
      "Iteration 929, loss = 0.04351388\n",
      "Iteration 930, loss = 0.04337673\n",
      "Iteration 931, loss = 0.04326989\n",
      "Iteration 932, loss = 0.04314711\n",
      "Iteration 933, loss = 0.04300011\n",
      "Iteration 934, loss = 0.04284345\n",
      "Iteration 935, loss = 0.04275118\n",
      "Iteration 936, loss = 0.04258239\n",
      "Iteration 937, loss = 0.04244127\n",
      "Iteration 938, loss = 0.04231717\n",
      "Iteration 939, loss = 0.04216328\n",
      "Iteration 940, loss = 0.04206754\n",
      "Iteration 941, loss = 0.04194908\n",
      "Iteration 942, loss = 0.04180601\n",
      "Iteration 943, loss = 0.04167460\n",
      "Iteration 944, loss = 0.04151504\n",
      "Iteration 945, loss = 0.04142808\n",
      "Iteration 946, loss = 0.04127831\n",
      "Iteration 947, loss = 0.04114075\n",
      "Iteration 948, loss = 0.04099195\n",
      "Iteration 949, loss = 0.04087806\n",
      "Iteration 950, loss = 0.04077113\n",
      "Iteration 951, loss = 0.04067233\n",
      "Iteration 952, loss = 0.04051665\n",
      "Iteration 953, loss = 0.04036828\n",
      "Iteration 954, loss = 0.04027014\n",
      "Iteration 955, loss = 0.04013627\n",
      "Iteration 956, loss = 0.04002304\n",
      "Iteration 957, loss = 0.03989209\n",
      "Iteration 958, loss = 0.03977209\n",
      "Iteration 959, loss = 0.03966838\n",
      "Iteration 960, loss = 0.03954276\n",
      "Iteration 961, loss = 0.03940241\n",
      "Iteration 962, loss = 0.03931062\n",
      "Iteration 963, loss = 0.03918172\n",
      "Iteration 964, loss = 0.03907126\n",
      "Iteration 965, loss = 0.03893117\n",
      "Iteration 966, loss = 0.03882479\n",
      "Iteration 967, loss = 0.03870758\n",
      "Iteration 968, loss = 0.03857692\n",
      "Iteration 969, loss = 0.03844630\n",
      "Iteration 970, loss = 0.03834893\n",
      "Iteration 971, loss = 0.03823708\n",
      "Iteration 972, loss = 0.03810658\n",
      "Iteration 973, loss = 0.03801897\n",
      "Iteration 974, loss = 0.03788535\n",
      "Iteration 975, loss = 0.03774289\n",
      "Iteration 976, loss = 0.03772148\n",
      "Iteration 977, loss = 0.03758709\n",
      "Iteration 978, loss = 0.03742109\n",
      "Iteration 979, loss = 0.03732917\n",
      "Iteration 980, loss = 0.03727122\n",
      "Iteration 981, loss = 0.03708896\n",
      "Iteration 982, loss = 0.03700179\n",
      "Iteration 983, loss = 0.03688028\n",
      "Iteration 984, loss = 0.03679196\n",
      "Iteration 985, loss = 0.03667294\n",
      "Iteration 986, loss = 0.03656240\n",
      "Iteration 987, loss = 0.03643065\n",
      "Iteration 988, loss = 0.03632591\n",
      "Iteration 989, loss = 0.03623297\n",
      "Iteration 990, loss = 0.03609880\n",
      "Iteration 991, loss = 0.03599300\n",
      "Iteration 992, loss = 0.03592833\n",
      "Iteration 993, loss = 0.03579778\n",
      "Iteration 994, loss = 0.03569436\n",
      "Iteration 995, loss = 0.03557596\n",
      "Iteration 996, loss = 0.03554998\n",
      "Iteration 997, loss = 0.03539549\n",
      "Iteration 998, loss = 0.03526644\n",
      "Iteration 999, loss = 0.03516524\n",
      "Iteration 1000, loss = 0.03504997\n",
      "Iteration 1001, loss = 0.03495220\n",
      "Iteration 1002, loss = 0.03485672\n",
      "Iteration 1003, loss = 0.03475349\n",
      "Iteration 1004, loss = 0.03468126\n",
      "Iteration 1005, loss = 0.03454948\n",
      "Iteration 1006, loss = 0.03447022\n",
      "Iteration 1007, loss = 0.03436547\n",
      "Iteration 1008, loss = 0.03425399\n",
      "Iteration 1009, loss = 0.03420655\n",
      "Iteration 1010, loss = 0.03408445\n",
      "Iteration 1011, loss = 0.03396417\n",
      "Iteration 1012, loss = 0.03386788\n",
      "Iteration 1013, loss = 0.03376808\n",
      "Iteration 1014, loss = 0.03366817\n",
      "Iteration 1015, loss = 0.03357585\n",
      "Iteration 1016, loss = 0.03350268\n",
      "Iteration 1017, loss = 0.03341211\n",
      "Iteration 1018, loss = 0.03331850\n",
      "Iteration 1019, loss = 0.03320858\n",
      "Iteration 1020, loss = 0.03309981\n",
      "Iteration 1021, loss = 0.03301249\n",
      "Iteration 1022, loss = 0.03291143\n",
      "Iteration 1023, loss = 0.03282824\n",
      "Iteration 1024, loss = 0.03275994\n",
      "Iteration 1025, loss = 0.03266186\n",
      "Iteration 1026, loss = 0.03256009\n",
      "Iteration 1027, loss = 0.03246071\n",
      "Iteration 1028, loss = 0.03237756\n",
      "Iteration 1029, loss = 0.03235463\n",
      "Iteration 1030, loss = 0.03220420\n",
      "Iteration 1031, loss = 0.03207523\n",
      "Iteration 1032, loss = 0.03201074\n",
      "Iteration 1033, loss = 0.03189938\n",
      "Iteration 1034, loss = 0.03184404\n",
      "Iteration 1035, loss = 0.03171745\n",
      "Iteration 1036, loss = 0.03161768\n",
      "Iteration 1037, loss = 0.03153720\n",
      "Iteration 1038, loss = 0.03147528\n",
      "Iteration 1039, loss = 0.03140306\n",
      "Iteration 1040, loss = 0.03129497\n",
      "Iteration 1041, loss = 0.03119551\n",
      "Iteration 1042, loss = 0.03110577\n",
      "Iteration 1043, loss = 0.03105210\n",
      "Iteration 1044, loss = 0.03100802\n",
      "Iteration 1045, loss = 0.03086981\n",
      "Iteration 1046, loss = 0.03079383\n",
      "Iteration 1047, loss = 0.03067700\n",
      "Iteration 1048, loss = 0.03061841\n",
      "Iteration 1049, loss = 0.03052856\n",
      "Iteration 1050, loss = 0.03041214\n",
      "Iteration 1051, loss = 0.03033822\n",
      "Iteration 1052, loss = 0.03027574\n",
      "Iteration 1053, loss = 0.03021081\n",
      "Iteration 1054, loss = 0.03017423\n",
      "Iteration 1055, loss = 0.03003777\n",
      "Iteration 1056, loss = 0.02991822\n",
      "Iteration 1057, loss = 0.02986087\n",
      "Iteration 1058, loss = 0.02977718\n",
      "Iteration 1059, loss = 0.02967600\n",
      "Iteration 1060, loss = 0.02960007\n",
      "Iteration 1061, loss = 0.02953876\n",
      "Iteration 1062, loss = 0.02948474\n",
      "Iteration 1063, loss = 0.02936128\n",
      "Iteration 1064, loss = 0.02928920\n",
      "Iteration 1065, loss = 0.02918928\n",
      "Iteration 1066, loss = 0.02912595\n",
      "Iteration 1067, loss = 0.02905188\n",
      "Iteration 1068, loss = 0.02893531\n",
      "Iteration 1069, loss = 0.02886764\n",
      "Iteration 1070, loss = 0.02878805\n",
      "Iteration 1071, loss = 0.02870193\n",
      "Iteration 1072, loss = 0.02861956\n",
      "Iteration 1073, loss = 0.02853341\n",
      "Iteration 1074, loss = 0.02848444\n",
      "Iteration 1075, loss = 0.02839110\n",
      "Iteration 1076, loss = 0.02830477\n",
      "Iteration 1077, loss = 0.02826859\n",
      "Iteration 1078, loss = 0.02815770\n",
      "Iteration 1079, loss = 0.02811611\n",
      "Iteration 1080, loss = 0.02801729\n",
      "Iteration 1081, loss = 0.02792418\n",
      "Iteration 1082, loss = 0.02784084\n",
      "Iteration 1083, loss = 0.02774320\n",
      "Iteration 1084, loss = 0.02768264\n",
      "Iteration 1085, loss = 0.02760513\n",
      "Iteration 1086, loss = 0.02755378\n",
      "Iteration 1087, loss = 0.02747852\n",
      "Iteration 1088, loss = 0.02738494\n",
      "Iteration 1089, loss = 0.02735724\n",
      "Iteration 1090, loss = 0.02722661\n",
      "Iteration 1091, loss = 0.02714925\n",
      "Iteration 1092, loss = 0.02710395\n",
      "Iteration 1093, loss = 0.02700954\n",
      "Iteration 1094, loss = 0.02694446\n",
      "Iteration 1095, loss = 0.02687602\n",
      "Iteration 1096, loss = 0.02679964\n",
      "Iteration 1097, loss = 0.02671711\n",
      "Iteration 1098, loss = 0.02664031\n",
      "Iteration 1099, loss = 0.02657042\n",
      "Iteration 1100, loss = 0.02650969\n",
      "Iteration 1101, loss = 0.02646179\n",
      "Iteration 1102, loss = 0.02639045\n",
      "Iteration 1103, loss = 0.02630462\n",
      "Iteration 1104, loss = 0.02622424\n",
      "Iteration 1105, loss = 0.02616739\n",
      "Iteration 1106, loss = 0.02608399\n",
      "Iteration 1107, loss = 0.02600329\n",
      "Iteration 1108, loss = 0.02598362\n",
      "Iteration 1109, loss = 0.02586549\n",
      "Iteration 1110, loss = 0.02580317\n",
      "Iteration 1111, loss = 0.02573834\n",
      "Iteration 1112, loss = 0.02565810\n",
      "Iteration 1113, loss = 0.02560834\n",
      "Iteration 1114, loss = 0.02557095\n",
      "Iteration 1115, loss = 0.02550678\n",
      "Iteration 1116, loss = 0.02541269\n",
      "Iteration 1117, loss = 0.02536095\n",
      "Iteration 1118, loss = 0.02526694\n",
      "Iteration 1119, loss = 0.02520289\n",
      "Iteration 1120, loss = 0.02513902\n",
      "Iteration 1121, loss = 0.02506941\n",
      "Iteration 1122, loss = 0.02499474\n",
      "Iteration 1123, loss = 0.02490990\n",
      "Iteration 1124, loss = 0.02489081\n",
      "Iteration 1125, loss = 0.02481364\n",
      "Iteration 1126, loss = 0.02478212\n",
      "Iteration 1127, loss = 0.02468598\n",
      "Iteration 1128, loss = 0.02461662\n",
      "Iteration 1129, loss = 0.02461022\n",
      "Iteration 1130, loss = 0.02448494\n",
      "Iteration 1131, loss = 0.02442362\n",
      "Iteration 1132, loss = 0.02439545\n",
      "Iteration 1133, loss = 0.02430293\n",
      "Iteration 1134, loss = 0.02425774\n",
      "Iteration 1135, loss = 0.02419515\n",
      "Iteration 1136, loss = 0.02409013\n",
      "Iteration 1137, loss = 0.02404986\n",
      "Iteration 1138, loss = 0.02408511\n",
      "Iteration 1139, loss = 0.02393817\n",
      "Iteration 1140, loss = 0.02389894\n",
      "Iteration 1141, loss = 0.02384446\n",
      "Iteration 1142, loss = 0.02377636\n",
      "Iteration 1143, loss = 0.02368882\n",
      "Iteration 1144, loss = 0.02367297\n",
      "Iteration 1145, loss = 0.02353171\n",
      "Iteration 1146, loss = 0.02354146\n",
      "Iteration 1147, loss = 0.02343804\n",
      "Iteration 1148, loss = 0.02340397\n",
      "Iteration 1149, loss = 0.02332296\n",
      "Iteration 1150, loss = 0.02326940\n",
      "Iteration 1151, loss = 0.02320671\n",
      "Iteration 1152, loss = 0.02316466\n",
      "Iteration 1153, loss = 0.02314545\n",
      "Iteration 1154, loss = 0.02302313\n",
      "Iteration 1155, loss = 0.02297910\n",
      "Iteration 1156, loss = 0.02293481\n",
      "Iteration 1157, loss = 0.02286619\n",
      "Iteration 1158, loss = 0.02280837\n",
      "Iteration 1159, loss = 0.02275673\n",
      "Iteration 1160, loss = 0.02271791\n",
      "Iteration 1161, loss = 0.02264509\n",
      "Iteration 1162, loss = 0.02258065\n",
      "Iteration 1163, loss = 0.02256088\n",
      "Iteration 1164, loss = 0.02248164\n",
      "Iteration 1165, loss = 0.02241116\n",
      "Iteration 1166, loss = 0.02235492\n",
      "Iteration 1167, loss = 0.02229602\n",
      "Iteration 1168, loss = 0.02227261\n",
      "Iteration 1169, loss = 0.02219362\n",
      "Iteration 1170, loss = 0.02214541\n",
      "Iteration 1171, loss = 0.02208796\n",
      "Iteration 1172, loss = 0.02202604\n",
      "Iteration 1173, loss = 0.02202406\n",
      "Iteration 1174, loss = 0.02192734\n",
      "Iteration 1175, loss = 0.02187507\n",
      "Iteration 1176, loss = 0.02181819\n",
      "Iteration 1177, loss = 0.02177102\n",
      "Iteration 1178, loss = 0.02171701\n",
      "Iteration 1179, loss = 0.02167091\n",
      "Iteration 1180, loss = 0.02161547\n",
      "Iteration 1181, loss = 0.02158193\n",
      "Iteration 1182, loss = 0.02151744\n",
      "Iteration 1183, loss = 0.02147121\n",
      "Iteration 1184, loss = 0.02137991\n",
      "Iteration 1185, loss = 0.02133643\n",
      "Iteration 1186, loss = 0.02133058\n",
      "Iteration 1187, loss = 0.02123938\n",
      "Iteration 1188, loss = 0.02118678\n",
      "Iteration 1189, loss = 0.02115162\n",
      "Iteration 1190, loss = 0.02112025\n",
      "Iteration 1191, loss = 0.02104228\n",
      "Iteration 1192, loss = 0.02099968\n",
      "Iteration 1193, loss = 0.02099173\n",
      "Iteration 1194, loss = 0.02090165\n",
      "Iteration 1195, loss = 0.02084504\n",
      "Iteration 1196, loss = 0.02079128\n",
      "Iteration 1197, loss = 0.02075112\n",
      "Iteration 1198, loss = 0.02069371\n",
      "Iteration 1199, loss = 0.02064063\n",
      "Iteration 1200, loss = 0.02060120\n",
      "Iteration 1201, loss = 0.02054052\n",
      "Iteration 1202, loss = 0.02047605\n",
      "Iteration 1203, loss = 0.02044314\n",
      "Iteration 1204, loss = 0.02042087\n",
      "Iteration 1205, loss = 0.02035074\n",
      "Iteration 1206, loss = 0.02029787\n",
      "Iteration 1207, loss = 0.02026759\n",
      "Iteration 1208, loss = 0.02022664\n",
      "Iteration 1209, loss = 0.02018553\n",
      "Iteration 1210, loss = 0.02011953\n",
      "Iteration 1211, loss = 0.02010197\n",
      "Iteration 1212, loss = 0.02005073\n",
      "Iteration 1213, loss = 0.01997547\n",
      "Iteration 1214, loss = 0.01993289\n",
      "Iteration 1215, loss = 0.01988211\n",
      "Iteration 1216, loss = 0.01982919\n",
      "Iteration 1217, loss = 0.01980314\n",
      "Iteration 1218, loss = 0.01977593\n",
      "Iteration 1219, loss = 0.01969196\n",
      "Iteration 1220, loss = 0.01966296\n",
      "Iteration 1221, loss = 0.01968527\n",
      "Iteration 1222, loss = 0.01956724\n",
      "Iteration 1223, loss = 0.01953382\n",
      "Iteration 1224, loss = 0.01949836\n",
      "Iteration 1225, loss = 0.01944009\n",
      "Iteration 1226, loss = 0.01939482\n",
      "Iteration 1227, loss = 0.01935622\n",
      "Iteration 1228, loss = 0.01928436\n",
      "Iteration 1229, loss = 0.01926224\n",
      "Iteration 1230, loss = 0.01920280\n",
      "Iteration 1231, loss = 0.01914998\n",
      "Iteration 1232, loss = 0.01912496\n",
      "Iteration 1233, loss = 0.01906288\n",
      "Iteration 1234, loss = 0.01902571\n",
      "Iteration 1235, loss = 0.01900159\n",
      "Iteration 1236, loss = 0.01896442\n",
      "Iteration 1237, loss = 0.01892428\n",
      "Iteration 1238, loss = 0.01885467\n",
      "Iteration 1239, loss = 0.01881438\n",
      "Iteration 1240, loss = 0.01878282\n",
      "Iteration 1241, loss = 0.01874000\n",
      "Iteration 1242, loss = 0.01870578\n",
      "Iteration 1243, loss = 0.01864966\n",
      "Iteration 1244, loss = 0.01861727\n",
      "Iteration 1245, loss = 0.01858761\n",
      "Iteration 1246, loss = 0.01856224\n",
      "Iteration 1247, loss = 0.01849791\n",
      "Iteration 1248, loss = 0.01844913\n",
      "Iteration 1249, loss = 0.01839704\n",
      "Iteration 1250, loss = 0.01834680\n",
      "Iteration 1251, loss = 0.01829746\n",
      "Iteration 1252, loss = 0.01827756\n",
      "Iteration 1253, loss = 0.01822850\n",
      "Iteration 1254, loss = 0.01819465\n",
      "Iteration 1255, loss = 0.01816740\n",
      "Iteration 1256, loss = 0.01812289\n",
      "Iteration 1257, loss = 0.01806958\n",
      "Iteration 1258, loss = 0.01803474\n",
      "Iteration 1259, loss = 0.01801430\n",
      "Iteration 1260, loss = 0.01798462\n",
      "Iteration 1261, loss = 0.01792175\n",
      "Iteration 1262, loss = 0.01789310\n",
      "Iteration 1263, loss = 0.01783705\n",
      "Iteration 1264, loss = 0.01778781\n",
      "Iteration 1265, loss = 0.01776942\n",
      "Iteration 1266, loss = 0.01772319\n",
      "Iteration 1267, loss = 0.01773827\n",
      "Iteration 1268, loss = 0.01764163\n",
      "Iteration 1269, loss = 0.01759910\n",
      "Iteration 1270, loss = 0.01757734\n",
      "Iteration 1271, loss = 0.01753483\n",
      "Iteration 1272, loss = 0.01748077\n",
      "Iteration 1273, loss = 0.01745480\n",
      "Iteration 1274, loss = 0.01741521\n",
      "Iteration 1275, loss = 0.01741188\n",
      "Iteration 1276, loss = 0.01733277\n",
      "Iteration 1277, loss = 0.01729197\n",
      "Iteration 1278, loss = 0.01727662\n",
      "Iteration 1279, loss = 0.01721958\n",
      "Iteration 1280, loss = 0.01719736\n",
      "Iteration 1281, loss = 0.01715596\n",
      "Iteration 1282, loss = 0.01712225\n",
      "Iteration 1283, loss = 0.01706123\n",
      "Iteration 1284, loss = 0.01707146\n",
      "Iteration 1285, loss = 0.01703919\n",
      "Iteration 1286, loss = 0.01696715\n",
      "Iteration 1287, loss = 0.01693942\n",
      "Iteration 1288, loss = 0.01688687\n",
      "Iteration 1289, loss = 0.01686350\n",
      "Iteration 1290, loss = 0.01684350\n",
      "Iteration 1291, loss = 0.01680232\n",
      "Iteration 1292, loss = 0.01675419\n",
      "Iteration 1293, loss = 0.01669939\n",
      "Iteration 1294, loss = 0.01667478\n",
      "Iteration 1295, loss = 0.01666563\n",
      "Iteration 1296, loss = 0.01661799\n",
      "Iteration 1297, loss = 0.01657410\n",
      "Iteration 1298, loss = 0.01651794\n",
      "Iteration 1299, loss = 0.01649274\n",
      "Iteration 1300, loss = 0.01645743\n",
      "Iteration 1301, loss = 0.01643528\n",
      "Iteration 1302, loss = 0.01639870\n",
      "Iteration 1303, loss = 0.01637868\n",
      "Iteration 1304, loss = 0.01633615\n",
      "Iteration 1305, loss = 0.01629158\n",
      "Iteration 1306, loss = 0.01624210\n",
      "Iteration 1307, loss = 0.01622257\n",
      "Iteration 1308, loss = 0.01619052\n",
      "Iteration 1309, loss = 0.01616729\n",
      "Iteration 1310, loss = 0.01614747\n",
      "Iteration 1311, loss = 0.01609499\n",
      "Iteration 1312, loss = 0.01604952\n",
      "Iteration 1313, loss = 0.01603278\n",
      "Iteration 1314, loss = 0.01599023\n",
      "Iteration 1315, loss = 0.01599383\n",
      "Iteration 1316, loss = 0.01591204\n",
      "Iteration 1317, loss = 0.01592619\n",
      "Iteration 1318, loss = 0.01586639\n",
      "Iteration 1319, loss = 0.01584385\n",
      "Iteration 1320, loss = 0.01578574\n",
      "Iteration 1321, loss = 0.01576767\n",
      "Iteration 1322, loss = 0.01572887\n",
      "Iteration 1323, loss = 0.01567939\n",
      "Iteration 1324, loss = 0.01565270\n",
      "Iteration 1325, loss = 0.01562351\n",
      "Iteration 1326, loss = 0.01561602\n",
      "Iteration 1327, loss = 0.01557651\n",
      "Iteration 1328, loss = 0.01552323\n",
      "Iteration 1329, loss = 0.01549122\n",
      "Iteration 1330, loss = 0.01546012\n",
      "Iteration 1331, loss = 0.01543500\n",
      "Iteration 1332, loss = 0.01541779\n",
      "Iteration 1333, loss = 0.01538567\n",
      "Iteration 1334, loss = 0.01535030\n",
      "Iteration 1335, loss = 0.01530257\n",
      "Iteration 1336, loss = 0.01528264\n",
      "Iteration 1337, loss = 0.01523439\n",
      "Iteration 1338, loss = 0.01520547\n",
      "Iteration 1339, loss = 0.01517947\n",
      "Iteration 1340, loss = 0.01513325\n",
      "Iteration 1341, loss = 0.01510707\n",
      "Iteration 1342, loss = 0.01508493\n",
      "Iteration 1343, loss = 0.01507152\n",
      "Iteration 1344, loss = 0.01503480\n",
      "Iteration 1345, loss = 0.01498309\n",
      "Iteration 1346, loss = 0.01497697\n",
      "Iteration 1347, loss = 0.01495355\n",
      "Iteration 1348, loss = 0.01492398\n",
      "Iteration 1349, loss = 0.01488389\n",
      "Iteration 1350, loss = 0.01484722\n",
      "Iteration 1351, loss = 0.01481342\n",
      "Iteration 1352, loss = 0.01480574\n",
      "Iteration 1353, loss = 0.01474601\n",
      "Iteration 1354, loss = 0.01474389\n",
      "Iteration 1355, loss = 0.01469335\n",
      "Iteration 1356, loss = 0.01465700\n",
      "Iteration 1357, loss = 0.01463112\n",
      "Iteration 1358, loss = 0.01462793\n",
      "Iteration 1359, loss = 0.01459510\n",
      "Iteration 1360, loss = 0.01454491\n",
      "Iteration 1361, loss = 0.01452015\n",
      "Iteration 1362, loss = 0.01450535\n",
      "Iteration 1363, loss = 0.01448301\n",
      "Iteration 1364, loss = 0.01446324\n",
      "Iteration 1365, loss = 0.01438008\n",
      "Iteration 1366, loss = 0.01441671\n",
      "Iteration 1367, loss = 0.01434417\n",
      "Iteration 1368, loss = 0.01430697\n",
      "Iteration 1369, loss = 0.01430305\n",
      "Iteration 1370, loss = 0.01425588\n",
      "Iteration 1371, loss = 0.01426123\n",
      "Iteration 1372, loss = 0.01419842\n",
      "Iteration 1373, loss = 0.01415680\n",
      "Iteration 1374, loss = 0.01416188\n",
      "Iteration 1375, loss = 0.01412834\n",
      "Iteration 1376, loss = 0.01409634\n",
      "Iteration 1377, loss = 0.01407058\n",
      "Iteration 1378, loss = 0.01404909\n",
      "Iteration 1379, loss = 0.01399149\n",
      "Iteration 1380, loss = 0.01396570\n",
      "Iteration 1381, loss = 0.01396288\n",
      "Iteration 1382, loss = 0.01393008\n",
      "Iteration 1383, loss = 0.01391444\n",
      "Iteration 1384, loss = 0.01386928\n",
      "Iteration 1385, loss = 0.01383838\n",
      "Iteration 1386, loss = 0.01381267\n",
      "Iteration 1387, loss = 0.01379991\n",
      "Iteration 1388, loss = 0.01378128\n",
      "Iteration 1389, loss = 0.01372148\n",
      "Iteration 1390, loss = 0.01374210\n",
      "Iteration 1391, loss = 0.01370168\n",
      "Iteration 1392, loss = 0.01365659\n",
      "Iteration 1393, loss = 0.01363731\n",
      "Iteration 1394, loss = 0.01360321\n",
      "Iteration 1395, loss = 0.01357047\n",
      "Iteration 1396, loss = 0.01354412\n",
      "Iteration 1397, loss = 0.01352738\n",
      "Iteration 1398, loss = 0.01348279\n",
      "Iteration 1399, loss = 0.01348565\n",
      "Iteration 1400, loss = 0.01343998\n",
      "Iteration 1401, loss = 0.01341092\n",
      "Iteration 1402, loss = 0.01338276\n",
      "Iteration 1403, loss = 0.01335070\n",
      "Iteration 1404, loss = 0.01333169\n",
      "Iteration 1405, loss = 0.01330299\n",
      "Iteration 1406, loss = 0.01327568\n",
      "Iteration 1407, loss = 0.01326004\n",
      "Iteration 1408, loss = 0.01322425\n",
      "Iteration 1409, loss = 0.01319941\n",
      "Iteration 1410, loss = 0.01317549\n",
      "Iteration 1411, loss = 0.01314737\n",
      "Iteration 1412, loss = 0.01312879\n",
      "Iteration 1413, loss = 0.01310098\n",
      "Iteration 1414, loss = 0.01309875\n",
      "Iteration 1415, loss = 0.01304978\n",
      "Iteration 1416, loss = 0.01304327\n",
      "Iteration 1417, loss = 0.01305490\n",
      "Iteration 1418, loss = 0.01299009\n",
      "Iteration 1419, loss = 0.01295443\n",
      "Iteration 1420, loss = 0.01293683\n",
      "Iteration 1421, loss = 0.01290717\n",
      "Iteration 1422, loss = 0.01288280\n",
      "Iteration 1423, loss = 0.01289394\n",
      "Iteration 1424, loss = 0.01283770\n",
      "Iteration 1425, loss = 0.01284212\n",
      "Iteration 1426, loss = 0.01279954\n",
      "Iteration 1427, loss = 0.01276002\n",
      "Iteration 1428, loss = 0.01275434\n",
      "Iteration 1429, loss = 0.01269602\n",
      "Iteration 1430, loss = 0.01267277\n",
      "Iteration 1431, loss = 0.01271307\n",
      "Iteration 1432, loss = 0.01266409\n",
      "Iteration 1433, loss = 0.01262704\n",
      "Iteration 1434, loss = 0.01259367\n",
      "Iteration 1435, loss = 0.01260034\n",
      "Iteration 1436, loss = 0.01255017\n",
      "Iteration 1437, loss = 0.01253760\n",
      "Iteration 1438, loss = 0.01250523\n",
      "Iteration 1439, loss = 0.01250895\n",
      "Iteration 1440, loss = 0.01247468\n",
      "Iteration 1441, loss = 0.01242753\n",
      "Iteration 1442, loss = 0.01239645\n",
      "Iteration 1443, loss = 0.01239144\n",
      "Iteration 1444, loss = 0.01238453\n",
      "Iteration 1445, loss = 0.01232798\n",
      "Iteration 1446, loss = 0.01232380\n",
      "Iteration 1447, loss = 0.01231568\n",
      "Iteration 1448, loss = 0.01229040\n",
      "Iteration 1449, loss = 0.01226693\n",
      "Iteration 1450, loss = 0.01227676\n",
      "Iteration 1451, loss = 0.01219542\n",
      "Iteration 1452, loss = 0.01221324\n",
      "Iteration 1453, loss = 0.01214591\n",
      "Iteration 1454, loss = 0.01215757\n",
      "Iteration 1455, loss = 0.01215628\n",
      "Iteration 1456, loss = 0.01208223\n",
      "Iteration 1457, loss = 0.01207274\n",
      "Iteration 1458, loss = 0.01205652\n",
      "Iteration 1459, loss = 0.01201839\n",
      "Iteration 1460, loss = 0.01198950\n",
      "Iteration 1461, loss = 0.01201575\n",
      "Iteration 1462, loss = 0.01196974\n",
      "Iteration 1463, loss = 0.01197228\n",
      "Iteration 1464, loss = 0.01193618\n",
      "Iteration 1465, loss = 0.01191187\n",
      "Iteration 1466, loss = 0.01188178\n",
      "Iteration 1467, loss = 0.01184499\n",
      "Iteration 1468, loss = 0.01185100\n",
      "Iteration 1469, loss = 0.01181051\n",
      "Iteration 1470, loss = 0.01179311\n",
      "Iteration 1471, loss = 0.01178930\n",
      "Iteration 1472, loss = 0.01175043\n",
      "Iteration 1473, loss = 0.01172592\n",
      "Iteration 1474, loss = 0.01170703\n",
      "Iteration 1475, loss = 0.01167700\n",
      "Iteration 1476, loss = 0.01168954\n",
      "Iteration 1477, loss = 0.01165008\n",
      "Iteration 1478, loss = 0.01164036\n",
      "Iteration 1479, loss = 0.01160454\n",
      "Iteration 1480, loss = 0.01161880\n",
      "Iteration 1481, loss = 0.01156459\n",
      "Iteration 1482, loss = 0.01154867\n",
      "Iteration 1483, loss = 0.01151871\n",
      "Iteration 1484, loss = 0.01149082\n",
      "Iteration 1485, loss = 0.01146522\n",
      "Iteration 1486, loss = 0.01146393\n",
      "Iteration 1487, loss = 0.01144695\n",
      "Iteration 1488, loss = 0.01142014\n",
      "Iteration 1489, loss = 0.01137909\n",
      "Iteration 1490, loss = 0.01142098\n",
      "Iteration 1491, loss = 0.01143936\n",
      "Iteration 1492, loss = 0.01134401\n",
      "Iteration 1493, loss = 0.01131173\n",
      "Iteration 1494, loss = 0.01127248\n",
      "Iteration 1495, loss = 0.01126316\n",
      "Iteration 1496, loss = 0.01123456\n",
      "Iteration 1497, loss = 0.01124911\n",
      "Iteration 1498, loss = 0.01124710\n",
      "Iteration 1499, loss = 0.01122621\n",
      "Iteration 1500, loss = 0.01117242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-8 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-8 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-8 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-8 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-8 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-8 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rn_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100, solver='adam', activation='relu', hidden_layer_sizes=(2, 2))\n",
    "rn_credit.fit(X_credit_treinamento, y_credit_treinamento)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rn_credit.predict(X_credit_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_credit_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy_score(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "cm = ConfusionMatrix(rn_credit)\n",
    "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
    "cm.score(X_credit_teste, y_credit_teste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      0.98      0.98        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       0.99      0.99      0.99       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_teste, previsoes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
